[section:lambert_w Lambert W function]

[h4:synopsis Synopsis]

``
#include <boost/math/special_functions/lambert_w.hpp>

``

   namespace boost { namespace math {

   template <class T>
   ``__sf_result`` lambert_w0(T z); // W0 branch, default precision.

   template <class T, class ``__Policy``>
   ``__sf_result`` lambert_w0(T z, const ``__Policy``&); // W0 policy controlled precision.

    template <class T>
   ``__sf_result`` lambert_wm1(T z); // W-1 branch.

   template <class T, class ``__Policy``>
   ``__sf_result`` lambert_wm1(T z, const ``__Policy``&); // W-1 policy controlled precision.

  } // namespace boost
  } // namespace math

[h4:description Description]

The __Lambert_W is the solution of the equation W[dot]e[super W] = z.
It is also called the Omega function, the inverse function of f(W) = W e[super W].

On the z-interval \[0, [infin]\] there is just one real solution.
On the interval (-1/e, 0) there are two real solutions on two branches called variously W0, W+, W-1, Wp, Wm, W-.
The principal branches called `lambert_W0` and `lambert_Wm1` are provided by this real-only implementation.

The function is described in Wolfram Mathworld as
[@http://mathworld.wolfram.com/LambertW-Function.html [^Lambert W function]].
The principal value of the Lambert W-function is implemented in the Wolfram Language as ['ProductLog\[z\]].

__WolframAlpha has provided some reference values for testing.

For example, the output from [@https://www.wolframalpha.com/input/?i=productlog(1)] is 0.56714329040978387299996866221035554975381578718651...

Also the [@https://www.wolframalpha.com Wolfram language] command:  [^N\[ProductLog\[-1\], 50\]]

produces the same output to 50 decimal digit precision.

The final __Policy argument is optional and can be used to control the behaviour of the function:
how it handles errors, what level of precision is wanted, etc.

Refer to __policy_section for more details and see examples below.

[h4:examples Examples]

[import ../../example/lambert_w_simple_examples.cpp]

You will need the following include and `using` statements:
[lambert_w_simple_examples_includes]

Some examples follow:

[lambert_w_simple_examples_0]

[lambert_w_simple_examples_1]

[lambert_w_simple_examples_2]

[lambert_w_simple_examples_3]

[warning When using multiprecision, take great care not to
construct or assign non-integers from `double` silently losing precision.]

[lambert_w_simple_examples_4]
Note the spurious non-zero decimal digits appearing after digit 17 in the argument 0.9000000000000000...!

See the correct result constructing from a decimal digit string:
[lambert_w_simple_examples_4a]
Note the expected zeros for all places up to 50, and the correct result.

Policies can be used to control precision (a little less precision gives a significant speedup):
[lambert_w_simple_examples_precision_policies]

and to control what action to take on errors:
[lambert_w_simple_examples_error_policies]

[tip Always use [^try'n'catch] blocks to ensure you get an error message like this:]


[lambert_w_simple_examples_error_message_1]

[lambert_w_simple_examples_out_of_range]

The full source of these examples is at [@../../example/lambert_w_example.cpp lambert_w_example.cpp]

[h4:precision Controlling the compromise between Precision and Speed]

The compromise between precision and speed can be controlled using __precision_policy.

Using the default policy, all the functions usually return values within two __ulp
for the floating-point type, except for very small arguments very near zero,
and for arguments very close to the singularity at the branch point.

By default, this implementation provides the best possible precision
(adding a __halley refinement, but without using __multiprecision higher precision types)
making it nearly twice as slow as Fukushima's algorithm,
(Boost.Math generally prefers accuracy over speed).
However, it will return at intermediate stages if the precision specified in the policy has been met,
usually at least halving the execution time.

It is convenient to define some `using` statements when using __policy_section to control precision.

    using boost::math::policies::policy;
    using boost::math::policies::precision;
    using boost::math::policies::digits2;  // Precision as bits.
    using boost::math::policies::digits10; // Precision as decimal digits.

[tip Because some macros and metaprogramming are used by the implementation of policies,
some ways of expressing these items may confuse the compiler and Visual Studio Intellisense.]

Precision targets can be specified in bits using `digits2` or decimal using `digits10`.
These are related by `digits10 = log10(2) * digits2 where log10(2) = 0.30103`.
Specifying `digits10` is a coarser measure and is roughly a third of `digits2`,
but may be slightly more intuitive than specifying precision in bits.

[import ../../example/lambert_w_precision_example.cpp]

To show the various stages the example below shows evaluation of Lambert W at varying precisions
specified using a policy with `digits10` increasing from 1 to 9.

For float this covers the full precision range as `max_digits10 = 9`.

[lambert_w_precision_1]
[lambert_w_precision_output_1]

If the policy precision is specified using `digits2`, one can see the small improvement from Halley at 22 bits:

  std::cout << lambert_w0(z, policy<digits2<21> >()) ...

  Lambert W (10.0000000, digits2<21>) = 1.74552798 << Schroeder.
  Lambert W (10.0000000, digits2<22>) = 1.74552810 << Halley.

For `double`, `max_digits10 == 17` and `digits == 53`, so the table is too long to here.

The final Schroeder refinement is skipped if `(digits2 <= (std::numeric_limits<T>::digits - 3))`
3 less than the maximum for the type `std::numeric_limits<T>::digits`.

For type `double`, the switch is at `policy<digits2<21> `, so this is recommended for general use when speed is important.

[tip Use `lambert_w0(z, policy<digits2<21> >())` if speed is more important than the ultimate precision.]

If speed is very important then `digits2<10>` only using Schroeder might suffice:

  Lambert W (10.0000000, digits2<10>) = 1.74414063 << Schroeder only.

For floating-point types with precision greater than __fundamental_types, a `double` evaluation is used
as a first approximation followed by Halley refinement,
so that there is is little point trying to control precision.
Higher precisions are always going to be [*very much slower].

For more examples of controlling precision, see [@../../include/boost/math/example/lambert_w_precision.cpp lambert_w_precision.cpp].

The implementation details of algorithm switch points are in
[@../../include/boost/math/special_functions/lambert_w.hpp lambert_w.hpp]

[h5:diagnostics Diagnostics Macros]

Several macros are provided to output diagnostic information (potentially [*much] output).
These can be statements like

`#define BOOST_MATH_INSTRUMENT_LAMBERT_W_TERMS`

placed [*before] the include statement

`#include <boost/math/special_functions/lambert_w.hpp>`,

or defined on the project compile command-line:  `/DBOOST_MATH_INSTRUMENT_LAMBERT_W_TERMS`,

or defined in a jamfile.v2:  `<define>BOOST_MATH_INSTRUMENT_LAMBERT_W_TERMS`

``
  BOOST_MATH_INSTRUMENT_LAMBERT_W0 // W0 branch diagnostics.
  BOOST_MATH_INSTRUMENT_LAMBERT_W1 // W1 branch diagnostics.
  BOOST_MATH_INSTRUMENT_LAMBERT_W_HALLEY // Halley refinement diagnostics.
  BOOST_MATH_INSTRUMENT_LAMBERT_W_SCHROEDER // Schroeder refinement diagnostics.
  BOOST_MATH_INSTRUMENT_LAMBERT_W_TERMS // Number of terms used for near-singularity series.
  BOOST_MATH_INSTRUMENT_LAMBERT_W0_NOT_BUILTIN // Higher than built-in precision types approximation and refinement.
  BOOST_MATH_INSTRUMENT_LAMBERT_W0_BISECTION // Show bisection only estimate.
  BOOST_MATH_INSTRUMENT_LAMBERT_W_SINGULARITY_SERIES  // Show result of estimates where z is near singularity where branches meet.
  BOOST_MATH_INSTRUMENT_LAMBERT_W_SMALL_Z_SERIES_ITERATIONS   // Show result of estimates where small z is near zero.
``

[h4:implemention Implementation]

There are many previous implementations with increasing accuracy and speed. See references below.

For most of the range of ['z] arguments, some initial approximation is followed by a single refinement,
often using Halley or similar method, and gives a useful precision.
To get a better precision, additional iterative refinements,
for example, using __halley or __schroder method, may be needed.

For the most precise results possible, for C++, close to the nearest representation for the type,
it is usually necessary to use a higher precision type for intermediate computation,
finally casting back to the smaller desired result type.
This strategy is used by Maple and Wolfram, for example, using arbitrary precision arithmetic,
and some of these high-precision values are used for testing this library.
This method is also used to provide __boost_test values using __multiprecision,
typically, a 50 decimal digit type like `cpp_dec_float_50` cast to a `float`, `double` or `long double` type.

For ['z]  argument values near the singularity and near zero, other approximations may be used,
possibly followed by refinement or increasing series terms until a desired precision is achieved.
At extreme arguments near to zero or the singularity at the branch point,
even this fails and the only method to achieve a really close result is to cast from a higher precision type.

In practical applications, the increased computation required
(often towards a thousand fold slower and requiring the additional code for multiprecision)
is not justified and the algorithms here do not implement this.
But because the Boost.Lambert_W algorithm has been tested using __multiprecision,
users who require this can always easily achieve the nearest representation for the __fundamental_types
if the application justifies the very large extra computation cost.

One real-only implementation was based on an algorithm by__Luu_thesis,
(see routine 11 on page 98 for his Lambert W algorithm)
and his Halley refinement is used in this implementation when required.

This implementation is based on Thomas Luu's code posted at
[@https://svn.boost.org/trac/boost/ticket/11027 Boost Trac \#11027].

It has been implemented from Luu's algorithm but templated on `RealType` parameter and result
and handles both __fundamental_types (`float, double, long double`), __multiprecision,
and also has been tested with a proposed fixed_point type.

A first approximation was computed using the method of Barry et al (see references 5 & 6 below).
(For users only requiring an accuracy of relative accuracy of 0.02%, this function might suffice).
This was extended to the widely used [@https://people.sc.fsu.edu/~jburkardt/f_src/toms443/toms443.html TOMS443]
FORTRAN and C++ versions by John Burkardt using Schroeder refinement(s).

We also considered using __newton method.
``
  f(w) = w e^w -z = 0 // Luu equation 6.37
  f'(w) = e^w (1 + w), Wolfram alpha (d)/(dw)(f(w) = w exp(w) - z) = e^w (w + 1)
  if (f(w) / f'(w) -1 < tolerance
  w1 = w0 - (expw0 * (w0 + 1)); // Refine new Newton/Raphson estimate.
``
but concluded that since Newton/Raphson's method takes typically 6 iterations to converge within tolerance,
whereas Halley usually takes only 1 to 3 iterations to achieve an result within 1 __ulp,
so Newton/Raphson's method unlikely to be quicker
than the additional cost of computating the 2nd derivative for Halley's method.

[h4:faster_implementation Implementing Faster Algorithms]

More recently, the Tosio Fukushima has developed faster algorithms,
avoiding any transcendental function calls as these are necessarily expensive.
The current implementation is based on his algorithm
starting with a translation from FORTRAN into C++ by Darko Veberic.

Many applications of the Lambert W function make many repeated evaluations for Monte Carlo methods,
for which applications speed is important.
Luu and Chapeau-Blondeau and Monir provide typical usage examples.

Fukushima makes the important observation that much of the execution time of all
previous iterative algorithms was spent evaluating transcendental functions, mainly `exp`.
He has put a lot of work into avoiding any slow transcendental functions by using lookup tables and
bisection, and finally by a single Schroeder refinement,
without any check on the precision of the result (necessarily evaluating an expensive exponential).

Theoretical and practical tests confirm that this gives Lambert W estimates
with a known small error bound over nearly all the range of ['z] argument.

However, though these give results within several epsilon of the nearest representable result,
they do not get as close as is often possible with further refinement, usually to within one or two epsilon.

For types more precise than `double`, Fukushima shows that it is best to use the `double` estimate
as a starting point followed by refinement using __halley iterations or other methods.

For this reason, the lookup tables and `bisection` are only carried out at low precision,
usually `double`, chosen by the `typedef double lookup_t`.  Unlike the FORTRAN version,
the lookup tables of Lambert_W of integral values are precomputed as C++ static arrays.

For the less well-behaved regions for ['z] arguments near zero and near the branch singularity at -1/e,
some series functions are provided.

So, as usual, there are compromises to consider between execution speed and accuracy.
This implementation allows users to get closer than Fukushima, at some small loss of speed
(but does not guarantee the nearest representable result)
or to get faster but less precise evalations controlled by __policies.

[h4:small_z  Small values of argument z near zero]

When argument ['z] is small and near zero, cancellation errors mean that accuracy is lost,
so other series function variants of `lambert_w0_small_z`.
(There is no equivalent for the W-1 branch as this only covers argument `z < -1/e`).
The cutoff used is as found by trial and error by Fukushima `abs(z) < 0.05`.

Coefficients of the inverted series expansion of the Lambert W function around `z = 0`
are computed following Fukushima using 17 terms of a Taylor series
computed using Wolfram with

  InverseSeries[Series[z Exp[z],{z,0,17}]]

See Tosio Fukushima, Journal of Computational and Applied Mathematics 244 (2013) page 86.

To provide higher precision constants (34 decimal digits) for types larger than `long double`,

   InverseSeries[Series[z Exp[z],{z,0,34}]]

were also computed.

Decimal values of specifications for built-in floating-point types below
are 21 digits precision == `std::numeric_limits<T>::max_digits10` for `long double`.

Specializations for `lambert_w0_small_z` are provided for
`float`, `double`, `long double`, `float128` and for __multiprecision types like .

The tag_type selection is based on the value `std::numeric_limits<T>::max_digits10`.
This distinguishes between `long double` types that commonly vary between 64 and 80-bits,
and also compilers that have a `float` type using 64 bits and/or `long double` using 128-bits.

[note One cannot switch tag_type on `std::numeric_limits<>::max()`
because comparison values may overflow the compiler limit.
Nor can we switch on `std::numeric_limits<long double>::max_exponent10()`
because both 80-bit and 128-bit floating-point types use 11 bits for exponent.
So must rely on `std::numeric_limits<long double>::max_digits10`.]

[warning It is assumes that `max_digits10` is defined correctly or this might fail to make the correct selection.
causing very small differences in computing lambert_w that would be very difficult to detect and diagnose.]

[warning The use of `doubledouble` types is untested and may give unexpected precision.]

As noted in the __lambert_w_implementation section above,
it is only possible ensure the nearest representable value by casting from a higher precision type,
computed at very, very much greater cost.

For multiprecision types, first several terms of the series are tabulated and evaluated as a polynomial:
(this will save us a bunch of expensive calls to `pow`).
Then our series functor is initialized "as if" it had already reached term 18,
enough evaluation of built-in 64-bit double and float (and 80-bit `long double`) types.
Finally the functor is called repeatedly to compute as many additional series terms
as necessary to achive the desired precision, set from `get_epsilon`.
Or terminated by `evaluation_error` on reaching the set iteration limit `max_series_iterations`.

A little more than one decimal digit of precision is gained by each additional series term.
This allows computation of Lambert W near zero to at least 1000 decimal digit precision,
given sufficient compute time.

[h4:near_singularity Argument z near the singularity at -1/e between branches W0 and W-1]

Variants of Function `lambert_w_singularity_series` are used to handle ['z] arguments
which are near to the singularity at `z = -exp(-1) = -3.6787944` where the branches W0 and W-1 join.

T. Fukushima / Journal of Computational and Applied Mathematics 244 (2013) Page 85, Table 3
described using Wolfram

InverseSeries\[Series\[sqrt\[2(p Exp\[1 + p\] + 1)\], {p,-1, 20}\]\]

to provide Table 3.

This implementation used Wolfram to obtain 40 series terms at 50 decimal digit precision

  N\[InverseSeries\[Series\[Sqrt\[2(p Exp\[1 + p\] + 1)\], { p,-1,40 }\]\], 50\]

  -1+p-p^2/3+(11 p^3)/72-(43 p^4)/540+(769 p^5)/17280-(221 p^6)/8505+(680863 p^7)/43545600 ...

These constants are computed at compile time for the full precision for any RealType T
using original rationals from Table 3.

Longer decimal digits strings are rationals pre-evaluated using Wolfram.
Some integer constants overflow, so use largest size available, suffixed by `uLL`.

Above the 14th term, the rationals exceed the range of `unsigned long long` and are replaced
by pre-computed decimal values at least 21 digits precision == `max_digits10` for `long double`.

A macro `BOOST_MATH_TEST_VALUE` taking a decimal floating-point literal was used
to allow use of both built-in floating-point types like `double`
which have contructors taking literal decimal values like `3.14`,
[*and] also multiprecision and other User-defined Types that only provide full-precision construction
from decimal digit strings like `"3.14"`.
(Construction of multiprecision types from built-in floating-point types
only provides the precision of the built-in type, like `double`, only 17 decimal digits).

[tip Be exceeding careful not to silently lose precision by constructing multiprecision types from literal decimal types, usually [^double].]

Fukushima used 20 series terms and it was confirmed that using more terms does not usefully increase accuracy.

[h4:testing Testing]

Initial testing of the algorithm was done using a small number of spot tests.

After it was established that the underlying algorithm (including unlimited Halley refinements with a tight terminating criterion) was correct,
some tables of Lambert W values were computed using a 100 decimal digit precision __multiprecision `cpp_dec_float_100` type and saved as
a C++ program that will initialise arrays of values of z arguments and lambert_W0 (`lambert_w_mp_high_values.ipp` and `lambert_w_mp_low_values.ipp` ).

(A few of these pairs were checked against values computed by Wolfram Alpha to try to guard against mistakes;
all those tested agreed to the penultimate decimal place, so they can be considered reliable to at least 98 decimal digits precision).

A macro `BOOST_MATH_TEST_VALUE` was used to allow tests with any real type, both __fundamental_types and __multiprecision.
(This is necessary because __fundamental_types have a constructor from floating-point literals like 3.1459F, 3.1459 or 3.1459L
whereas __multiprecision types may lose precision unless constructed from decimal digits strings like "3.1459").

The 100-decimal digits precision pairs were then used to assess the precision of less-precise types, including
__multiprecision `cpp_bin_float_quad` and `cpp_bin_float_50`.  `static_cast`ing from the high precision types should
give the closest representable value of the less-precise type; this is then be used to assess the precision of
the Lambert W algorithm.  For __fundamental_types, the precision requirement set by the __Policy mean that the algorithm
may stop refinement after bisection, or Schroeder or Halley refinements.

With the default __Policy, that includes iterative Halley refinement, tests confirm that over nearly all the range of z arguments,
nearly all estimates are the nearest __representable value, a minority are within 1 __ulp and only a very few 2 ULP.

For the range of z arguments over the range -0.35 to 0.5, a different algorithm is used, but the same
technique of evaluating reference values using a __multiprecision `cpp_dec_float_100` was used.
For extremely small z arguments, near zero, and those extremely near the singularity at the branch point,
precision can be much lower, as might be expected.


[h5 Other implementations]

The Lambert W has also been discussed in a [@http://lists.boost.org/Archives/boost/2016/09/230819.php Boost thread].

This also gives link to a prototype version by which also gives complex results [^(x < -exp(-1)], about -0.367879).
[@https://github.com/CzB404/lambert_w/ Balazs Cziraki 2016]
Physicist, PhD student at Eotvos Lorand University, ELTE TTK Institute of Physics, Budapest.
has also produced a prototype C++ library that can compute the Lambert W function for floating point
[*and complex number types].
This is not implemented here but might be completed in the future.

[h4:acknowledgements  Acknowledgements]

* Thanks to Wolfram for use of their invaluable online Wolfram Alpha service.
* Thanks for Mark Chapman for performing offline Wolfram computations.

[h4:references References]

# NIST Digital Library of Mathematical Functions. [@http://dlmf.nist.gov/4.13.F1].

# [@http://www.orcca.on.ca/LambertW/ Lambert W Poster],
R. M. Corless, G. H. Gonnet, D. E. G. Hare, D. J. Jeffery and D. E. Knuth,
On the Lambert W function Advances in Computational Mathematics, Vol 5, (1996) pp 329-359.

# [@https://people.sc.fsu.edu/~jburkardt/f_src/toms443/toms443.html TOMS443],
Andrew Barry, S. J. Barry, Patricia Culligan-Hensley,
Algorithm 743: WAPR - A Fortran routine for calculating real values of the W-function,[br]
ACM Transactions on Mathematical Software, Volume 21, Number 2, June 1995, pages 172-181.[br]
BISECT approximates the W function using bisection (GNU licence).
Original FORTRAN77 version by Andrew Barry, S. J. Barry, Patricia Culligan-Hensley,
this version by C++ version by John Burkardt.

# [@https://people.sc.fsu.edu/~jburkardt/f_src/toms743/toms743.html TOMS743] Fortran 90 (updated 2014).

Initial guesses based on:

# D.A. Barry, J.-Y. Parlange, L. Li, H. Prommer, C.J. Cunningham, and
F. Stagnitti. Analytical approximations for real values of the Lambert
W-function. Mathematics and Computers in Simulation, 53(1), 95-103 (2000).

# D.A. Barry, J.-Y. Parlange, L. Li, H. Prommer, C.J. Cunningham, and
F. Stagnitti. Erratum to analytical approximations for real values of the
Lambert W-function. Mathematics and Computers in Simulation, 59(6):543-543, 2002.

# C++ __CUDA version of Luu algorithm, [@https://github.com/thomasluu/plog/blob/master/plog.cu plog].

# __Luu_thesis, see routine 11, page 98 for Lambert W algorithm.

# Having Fun with Lambert W(x) Function, Darko Veberic
University of Nova Gorica, Slovenia IK, Forschungszentrum Karlsruhe, Germany, J. Stefan Institute, Ljubljana, Slovenia.

# Fran[ccedil]ois Chapeau-Blondeau and Abdelilah Monir, Numerical Evaluation of the Lambert W Function and Application to Generation of Generalized
Gaussian Noise With Exponent 1/2, IEEE Transactions on Signal Processing, 50(9) (2002) 2160 - 2165.

# Toshio Fukushima, Precise and fast computation of Lambert W-functions without transcendental function evaluations, Journal of Computational and Applied
Mathematics, 244 (2013) 77-89.

[endsect] [/section:lambert_w Lambert W function]

[/
  Copyright 2016 John Maddock, Paul A. Bristow, Thomas Luu.
  Distributed under the Boost Software License, Version 1.0.
  (See accompanying file LICENSE_1_0.txt
  or copy at http://www.boost.org/LICENSE_1_0.txt).
]
